// services/FileKnowledgeService.js
const pdf = require('pdf-parse');
const mammoth = require('mammoth');
const Tesseract = require('tesseract.js');
const AdmZip = require('adm-zip');
const TurndownService = require('turndown');
const { gfm } = require('turndown-plugin-gfm'); // H·ªó tr·ª£ Table trong Markdown
const axios = require('axios');
const { JSDOM } = require('jsdom');
const { Readability } = require('@mozilla/readability');
const deepseekService = require('./deepseekService');
const crypto = require('crypto');

class FileKnowledgeService {

    constructor() {
        this.turndownService = new TurndownService({
            headingStyle: 'atx',
            codeBlockStyle: 'fenced'
        });
        this.turndownService.use(gfm);

        // Tunable parameters
        this.MAX_CHARS = 4000;       // K√≠ch th∆∞·ªõc chunk ch√≠nh
        this.OVERLAP_CHARS = 600;    // Overlap ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh
        this.MIN_CHUNK = 200;        // N·∫øu ƒëo·∫°n qu√° ng·∫Øn -> b·ªè
    }

    async processInput(input) {
        if (input.buffer && input.mimetype) {
            return await this.extractTextFromFile(input);
        } else if (typeof input === 'string' && input.startsWith('http')) {
            return await this.extractTextFromUrl(input);
        }
        throw new Error("ƒê·ªãnh d·∫°ng ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá");
    }

    async extractTextFromUrl(url) {
        try {
            console.log(`üåê ƒêang c√†o d·ªØ li·ªáu t·ª´: ${url}`);
            const { data } = await axios.get(url, {
                headers: { 'User-Agent': 'Mozilla/5.0' }
            });

            const dom = new JSDOM(data, { url });
            const reader = new Readability(dom.window.document);
            const article = reader.parse();

            if (!article) return "";

            const markdownContent = this.turndownService.turndown(article.content);
            const fullText = `Ngu·ªìn: ${url}\nTi√™u ƒë·ªÅ: ${article.title}\n\n${markdownContent}`;

            // Tr·∫£ v·ªÅ ~m·ªôt object ch·ª©a text v√† metadata ƒë·ªÉ downstream chunking
            return {
                text: fullText,
                meta: { source: url, title: article.title }
            };
        } catch (error) {
            console.error("URL Parse Error:", error.message);
            throw new Error("Kh√¥ng th·ªÉ ƒë·ªçc n·ªôi dung t·ª´ ƒë∆∞·ªùng d·∫´n n√†y.");
        }
    }

    async extractTextFromFile(file) {
        const buffer = file.buffer;
        const mimeType = file.mimetype;

        try {
            if (mimeType === 'application/pdf') {
                const text = await this.processPdf(buffer);
                return { text, meta: { filename: file.originalname, mimeType } };
            } else if (mimeType === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document') {
                const markdown = await this.processDocx(buffer);
                return { text: markdown, meta: { filename: file.originalname, mimeType } };
            } else if (mimeType.startsWith('text/')) {
                return { text: buffer.toString('utf-8'), meta: { filename: file.originalname, mimeType } };
            } else if (mimeType.startsWith('image/')) {
                const text = await this.processImage(buffer);
                return { text, meta: { filename: file.originalname, mimeType } };
            }
            return { text: "", meta: { filename: file.originalname, mimeType } };
        } catch (error) {
            console.error("File Parse Error:", error);
            throw new Error("L·ªói ƒë·ªçc file: " + error.message);
        }
    }

    async processDocx(buffer) {
        let finalMarkdown = "";

        try {
            const { value: html } = await mammoth.convertToHtml({ buffer: buffer });
            finalMarkdown += this.turndownService.turndown(html);
        } catch (e) {
            console.warn("Mammoth error:", e.message);
        }

        try {
            const zip = new AdmZip(buffer);
            const zipEntries = zip.getEntries();
            const imageEntries = zipEntries.filter(entry =>
                entry.entryName.startsWith('word/media/') &&
                /\.(png|jpg|jpeg|bmp)$/i.test(entry.name)
            );

            if (imageEntries.length > 0) {
                console.log(`üì∏ DOCX: T√¨m th·∫•y ${imageEntries.length} ·∫£nh. ƒêang OCR...`);
                const ocrTexts = await Promise.all(imageEntries.map(async (entry) => {
                    if (entry.getData().length > 5000) {
                        return await this.processImage(entry.getData());
                    }
                    return "";
                }));

                const validOcr = ocrTexts.filter(t => t && t.trim().length > 10).join("\n\n");
                if (validOcr) {
                    finalMarkdown += `\n\n## [N·ªòI DUNG T·ª™ H√åNH ·∫¢NH]\n${validOcr}`;
                }
            }
        } catch (e) {
            console.error("DOCX Image Error:", e.message);
        }

        return finalMarkdown;
    }

    async processPdf(buffer) {
        // L·∫•y text b·∫±ng pdf-parse; n·∫øu ng·∫Øn -> b√°o ƒë·ªÉ c√≥ th·ªÉ c√¢n nh·∫Øc OCR trang PDF (n√¢ng c·∫•p)
        const data = await pdf(buffer);
        const txt = (data && data.text) ? data.text.trim() : "";

        // N·∫øu pdf-parse tr·∫£ v·ªÅ qu√° ng·∫Øn -> c·∫£nh b√°o (n√™n rasterize pages -> OCR). 
        // ·ªû ƒë√¢y ta tr·∫£ v·ªÅ txt (c√≥ th·ªÉ r·ªóng). B·∫°n c√≥ th·ªÉ n√¢ng c·∫•p th√™m b·∫±ng pdf2pic / pdf-lib ƒë·ªÉ rasterize -> Tesseract.
        if (!txt || txt.length < 100) {
            console.warn('[PDF] C√≥ v·∫ª l√† PDF scan ho·∫∑c pdf-parse tr·∫£ v·ªÅ √≠t text. Xem x√©t b·∫≠t OCR trang PDF (pdf2pic -> tesseract).');
        }

        return txt;
    }

    async processImage(buffer) {
        try {
            const { data: { text } } = await Tesseract.recognize(buffer, 'vie+eng');
            return text;
        } catch (e) {
            console.warn('OCR image l·ªói', e.message);
            return "";
        }
    }

    // =============== Chunking c·∫£i ti·∫øn ===============
    /**
     * Tr·∫£ v·ªÅ m·∫£ng chunks v·ªõi metadata:
     * [{ chunkId, text, start, end, chunkIndex, sourceMeta }]
     */
    async generateChunksFromText(raw) {
        const rawText = (typeof raw === 'string') ? raw : (raw && raw.text) ? raw.text : '';
        const sourceMeta = (raw && raw.meta) ? raw.meta : {};

        if (!rawText || rawText.trim().length < 50) return [];

        // 1) Pre-clean: remove common headers/footers (s·ªë trang, header c·ªßa site...). ƒê√¢y l√† heuristic.
        const cleanedStep1 = this._cleanHeadersFooters(rawText);


        // 1.5) **M·ªöI:** Lo·∫°i b·ªè base64 v√† d·ªØ li·ªáu r√°c
        const cleaned = this._removeNoiseAndGarbage(cleanedStep1);

        // 2) Split by headings / section markers first (n·∫øu c√≥)
        const sectionCandidates = this._splitByHeadings(cleaned);

        // 3) For each candidate, sub-split using sliding window and keep overlap
        const chunks = [];
        let chunkIndex = 0;
        for (const sec of sectionCandidates) {
            const secTrim = sec.trim();
            if (secTrim.length < this.MIN_CHUNK) continue;

            // sliding window
            let start = 0;
            while (start < secTrim.length) {
                const end = Math.min(start + this.MAX_CHARS, secTrim.length);
                const piece = secTrim.slice(start, end).trim();

                if (piece.length >= this.MIN_CHUNK) {
                    const chunkId = this._makeId(sourceMeta.filename || sourceMeta.source || 'text', chunkIndex, start);
                    chunks.push({
                        chunkId,
                        text: piece,
                        start,
                        end,
                        chunkIndex,
                        sourceMeta
                    });
                    chunkIndex++;
                }

                if (end === secTrim.length) break;
                // move window with overlap
                start = Math.max(0, end - this.OVERLAP_CHARS);
            }
        }

        // If nothing produced (edge-case), fallback to hard split
        if (chunks.length === 0 && cleaned.length > 0) {
            for (let i = 0, idx = 0; i < cleaned.length; i += (this.MAX_CHARS - this.OVERLAP_CHARS), idx++) {
                const piece = cleaned.substring(i, Math.min(i + this.MAX_CHARS, cleaned.length)).trim();
                if (piece.length >= this.MIN_CHUNK) {
                    chunks.push({
                        chunkId: this._makeId(sourceMeta.filename || sourceMeta.source || 'text', idx, i),
                        text: piece,
                        start: i,
                        end: Math.min(i + this.MAX_CHARS, cleaned.length),
                        chunkIndex: idx,
                        sourceMeta
                    });
                }
            }
        }

        console.log(`üîπ T·ªïng ƒë·ªô d√†i: ${cleaned.length} chars. Chia th√†nh ${chunks.length} ph·∫ßn.`);

        // 4) Call AI per chunk (th·ª±c t·∫ø n√™n batch / concurrency limit)
        const allKnowledge = [];
        for (let i = 0; i < chunks.length; i++) {
            const c = chunks[i];
            console.log(`‚è≥ Extracing chunk ${i + 1}/${chunks.length}`);
            try {
                const items = await this.processSingleChunkWithAI(c);
                // attach provenance
                (items || []).forEach(it => {
                    it._provenance = {
                        chunkId: c.chunkId,
                        chunkIndex: c.chunkIndex,
                        start: c.start,
                        end: c.end,
                        sourceMeta: c.sourceMeta
                    };
                });
                if (Array.isArray(items) && items.length) allKnowledge.push(...items);
            } catch (e) {
                console.warn('L·ªói AI extract chunk:', e.message);
            }
        }

        // 5) Merge items by entityId/canonicalId (n·∫øu AI tr·∫£) + dedupe small items
        const merged = this.mergeKnowledgeItems(allKnowledge);

        return merged;
    }

    _cleanHeadersFooters(text) {
        // Lo·∫°i b·ªè lines ki·ªÉu "Page 1 of 10" ho·∫∑c "Trang 1/10" ho·∫∑c header heavy
        // Gi·ªØ l·∫°i c√°c regex l√†m s·∫°ch c≈©
        return text
            .replace(/\n?Page\s*\d+\s*(of\s*\d+)?\s*\n?/ig, '\n')
            .replace(/\n?Trang\s*\d+\/\d+\s*\n?/ig, '\n')
            .replace(/\r\n/g, '\n')
            .replace(/\t/g, ' ')
            .replace(/[ ]{2,}/g, ' ');
    }

    _removeNoiseAndGarbage(text) {
        // Regex ƒë·ªÉ t√¨m ki·∫øm c√°c chu·ªói base64 d√†i (th∆∞·ªùng do h√¨nh ·∫£nh/binary kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω)
        // ƒê√¢y l√† regex heuristic, t√¨m chu·ªói √≠t nh·∫•t 50 k√Ω t·ª± A-Za-z0-9+/=
        // L∆∞u √Ω: C√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh ƒë·ªô d√†i t·ªëi thi·ªÉu (50) t√πy theo d·ªØ li·ªáu th·ª±c t·∫ø.
        const base64Regex = /([A-Za-z0-9+/=]{50,})[\s\n]*/g;

        let cleaned = text.replace(base64Regex, (match, p1) => {
            // Ch·ªâ lo·∫°i b·ªè n·∫øu chu·ªói kh√¥ng ph·∫£i l√† m·ªôt ƒëo·∫°n code h·ª£p l√Ω (heuristic)
            if (p1.length > 100 && !p1.includes(' ')) {
                console.log(`[Cleaner] ƒê√£ lo·∫°i b·ªè chu·ªói base64 d√†i (len: ${p1.length})`);
                return '\n'; // Thay th·∫ø b·∫±ng xu·ªëng d√≤ng ƒë·ªÉ tr√°nh d√≠nh li·ªÅn n·ªôi dung
            }
            return match; // Gi·ªØ l·∫°i n·∫øu l√† chu·ªói ng·∫Øn ho·∫∑c c√≥ v·∫ª l√† code
        });

        // Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒëi·ªÅu khi·ªÉn/ASCII kh√¥ng in ƒë∆∞·ª£c (tr·ª´ \n)
        cleaned = cleaned.replace(/[\x00-\x1F\x7F-\x9F]/g, '');

        // Lo·∫°i b·ªè c√°c d√≤ng ch·ªâ ch·ª©a k√Ω t·ª± r√°c/ƒë√°nh d·∫•u kh√¥ng li√™n quan
        cleaned = cleaned.split('\n').filter(line => line.trim().length > 3 || line.trim().length === 0).join('\n');

        return cleaned;
    }

    _splitByHeadings(text) {
        // Chia theo c√°c heading markdown (# ), ho·∫∑c d√≤ng vi·∫øt hoa ho·∫∑c d√≤ng k·∫øt th√∫c b·∫±ng ':' (Ti√™u ƒë·ªÅ:)
        const parts = [];
        // Try split by markdown headings first
        const mdSections = text.split(/\n(?=#+)/);
        if (mdSections.length > 1) return mdSections;

        // If no markdown, split by double newlines but keep lines which look like headings
        const paragraphs = text.split(/\n\s*\n/);
        let current = '';
        for (const p of paragraphs) {
            const trimmed = p.trim();
            const isHeading = /^#{1,6}\s+/.test(trimmed) || /^[A-Z0-9\s\-\,\(\)\/]{10,}$/.test(trimmed) || /:$/i.test(trimmed);
            if (isHeading && current.trim().length > 0) {
                parts.push(current);
                current = trimmed + '\n\n';
            } else {
                current += '\n\n' + trimmed;
            }
        }
        if (current.trim()) parts.push(current);
        return parts;
    }

    _makeId(prefix, idx, pos) {
        return `${prefix}-${idx}-${pos}-${crypto.createHash('md5').update(prefix + idx + pos).digest('hex').slice(0, 6)}`;
    }

    /**
     * G·ªçi AI x·ª≠ l√Ω 1 chunk -> tr·∫£ v·ªÅ m·∫£ng tri th·ª©c
     * Y√™u c·∫ßu AI tr·∫£ v·ªÅ JSON array g·ªìm object c√≥ fields:
     * { "entityId": "canonical id or name", "title": "", "content": "", "keywords": [], "type": "entity|fact|table" }
     */
    async processSingleChunkWithAI(chunk) {
        const textSegment = chunk.text;
        const prompt = `
B·∫°n l√† m·ªôt extractor chuy√™n nghi·ªáp cho tri th·ª©c (RAG).
NHI·ªÜM V·ª§: T·ª´ ƒëo·∫°n vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y, tr√≠ch xu·∫•t c√°c m·∫©u tri th·ª©c ƒë·ªôc l·∫≠p (n·∫øu c√≥). M·ªói m·∫©u tri th·ª©c n√™n m√¥ t·∫£ 1 "entity" ho·∫∑c 1 fact ho√†n ch·ªânh.
Y√äU C·∫¶U CH·∫§T L∆Ø·ª¢NG V√Ä H√åNH TH·ª®C:
1) PH·∫¢I TR·∫¢ V·ªÄ **CH·ªà** 1 M·∫¢NG JSON (JSON array). KH√îNG N√ìI TH√äM, KH√îNG GI·∫¢I TH√çCH.
2) M·ªñI M·∫¢NH TRI TH·ª®C PH·∫¢I **B·∫¢O TO√ÄN √ù NGHƒ®A v√† T√çNH CH√çNH X√ÅC CAO** so v·ªõi n·ªôi dung g·ªëc. ƒê·ª´ng t√≥m t·∫Øt qu√° ng·∫Øn l√†m m·∫•t ƒëi ng·ªØ c·∫£nh quan tr·ªçng.
3) N·∫æU ƒêO·∫†N VƒÇN B·∫¢N CH·ª®A D·ªÆ LI·ªÜU R√ÅC (v√≠ d·ª•: chu·ªói m√£ h√≥a base64 d√†i, m√£ HTML b·ªã l·ªói, k√Ω t·ª± kh√¥ng li√™n quan, ho·∫∑c ch·ªâ l√† footer/header r·ªóng) -> **KH√îNG TR√çCH XU·∫§T** v√† tr·∫£ v·ªÅ **[]** (m·∫£ng r·ªóng).
4) M·ªói ph·∫ßn t·ª≠ trong m·∫£ng c√≥ ƒë·ªãnh d·∫°ng:
   {
     "entityId": "chu·ªói ƒë·ªãnh danh ti√™u chu·∫©n (n·∫øu c√≥ th·ªÉ, ƒë·∫∑t t√™n canonical ‚Äî v√≠ d·ª•: 'C√¥ng ty ABC', ho·∫∑c 'S·∫£n ph·∫©m XYZ' ‚Äî n·∫øu kh√¥ng bi·∫øt, ƒë·ªÉ r·ªóng string)",
     "title": "Ti√™u ƒë·ªÅ ng·∫Øn t√≥m t·∫Øt m·∫©u tri th·ª©c",
     "content": "N·ªôi dung chi ti·∫øt (1-4 c√¢u) m√¥ t·∫£ m·∫©u tri th·ª©c n√†y. PH·∫¢I ƒê·ª¶ √ù, kh√¥ng bao g·ªìm th√¥ng tin th·ª´a nh∆∞ s·ªë trang, header, footer.",
     "keywords": ["t·ª´ kh√≥a 1", "t·ª´ kh√≥a 2"],
     "type": "entity" | "fact" | "table",
     "confidence": 0.0  // Gi√° tr·ªã 0..1 do model estimate (t√πy ch·ªçn)
   }

5) N·∫øu th·∫•y b·∫£ng (table), c·ªë g·∫Øng chuy·ªÉn sang JSON ho·∫∑c m√¥ t·∫£ b·∫£ng b·∫±ng list.
6) N·∫øu th√¥ng tin thu·ªôc c√πng 1 entity xu·∫•t hi·ªán nhi·ªÅu chunk, ƒë·∫£m b·∫£o entityId nh·∫•t qu√°n (ƒë·∫∑t canonical name).

INPUT:
"""
${textSegment}
"""

OUTPUT: (v√≠ d·ª•)
[
  { "entityId": "C√¥ng ty ABC", "title": "M√¥ t·∫£ c√¥ng ty ABC", "content": "C√¥ng ty ABC l√† ...", "keywords": ["ABC","c√¥ng ty"], "type":"entity", "confidence":0.9 }
]
`;

        try {
            const aiResponse = await deepseekService.chat([
                { role: 'system', content: 'Strict JSON Output Agent.' },
                { role: 'user', content: prompt }
            ], {
                temperature: 0.0,
                max_tokens: 1500
            });

            // aiResponse c√≥ th·ªÉ l√† string ho·∫∑c object; chu·∫©n h√≥a th√†nh string
            const raw = (typeof aiResponse === 'string') ? aiResponse : (aiResponse && aiResponse.content) ? aiResponse.content : JSON.stringify(aiResponse);

            const jsonStr = this._extractFirstJsonArray(raw);
            if (!jsonStr) return [];
            const parsed = JSON.parse(jsonStr);

            // Ensure each item has minimal fields
            return parsed.map(item => ({
                entityId: (item.entityId || item.title || '').toString().trim(),
                title: (item.title || '').toString().trim(),
                content: (item.content || '').toString().trim(),
                keywords: Array.isArray(item.keywords) ? item.keywords.map(k => k.toString()) : [],
                type: item.type || 'fact',
                confidence: (typeof item.confidence === 'number') ? item.confidence : 0.8
            }));

        } catch (error) {
            console.warn("AI Segment Error (Skipping):", error.message);
            return [];
        }
    }

    _extractFirstJsonArray(s) {
        // T√¨m d·∫•u '[' ƒë·∫ßu ti√™n v√† ']' t∆∞∆°ng ·ª©ng c√¢n b·∫±ng
        const first = s.indexOf('[');
        if (first === -1) return null;
        let depth = 0;
        for (let i = first; i < s.length; i++) {
            if (s[i] === '[') depth++;
            else if (s[i] === ']') {
                depth--;
                if (depth === 0) {
                    return s.substring(first, i + 1);
                }
            }
        }
        return null;
    }

    /**
     * Merge knowledge items:
     * - N·∫øu entityId t·ªìn t·∫°i -> group v√† concat content, merge keywords, keep best title/confidence
     * - N·∫øu kh√¥ng -> gi·ªØ nguy√™n (c√≥ th·ªÉ later d√πng embedding clustering)
     */
    mergeKnowledgeItems(items) {
        if (!Array.isArray(items) || items.length === 0) return [];

        const byEntity = new Map();
        const noEntity = [];

        for (const it of items) {
            const id = (it.entityId || '').trim();
            if (id) {
                const key = id.toLowerCase();
                if (!byEntity.has(key)) {
                    byEntity.set(key, { entityId: it.entityId, title: it.title || '', content: it.content || '', keywords: new Set(it.keywords || []), type: it.type || 'entity', confidence: it.confidence || 0 });
                } else {
                    const cur = byEntity.get(key);
                    // concat content with separation and dedupe small duplicates
                    if (!cur.content.includes(it.content)) {
                        cur.content = cur.content + "\n\n" + it.content;
                    }
                    it.keywords && it.keywords.forEach(k => cur.keywords.add(k));
                    if ((it.title || '').length > (cur.title || '').length) cur.title = it.title;
                    cur.confidence = Math.max(cur.confidence, it.confidence || 0);
                }
            } else {
                noEntity.push(it);
            }
        }

        const merged = [];
        for (const [k, v] of byEntity.entries()) {
            merged.push({
                entityId: v.entityId,
                title: v.title,
                content: v.content,
                keywords: Array.from(v.keywords),
                type: v.type,
                confidence: v.confidence
            });
        }

        // append the noEntity items (optionally de-duplicate by content)
        // Simple dedupe: remove items whose content is contained by merged entity content
        for (const it of noEntity) {
            const dup = merged.find(m => m.content && it.content && m.content.includes(it.content));
            if (!dup) merged.push(it);
        }

        return merged;
    }
}

module.exports = new FileKnowledgeService();
